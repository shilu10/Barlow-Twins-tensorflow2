{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"source":["try:\n","    from imutils import paths \n","except:\n","    !pip install imutils \n","    from imutils import paths "]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2023-05-11T12:32:46.246555Z","iopub.status.busy":"2023-05-11T12:32:46.246152Z","iopub.status.idle":"2023-05-11T12:32:46.882849Z","shell.execute_reply":"2023-05-11T12:32:46.880707Z","shell.execute_reply.started":"2023-05-11T12:32:46.246527Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n","\n","TensorFlow Addons (TFA) has ended development and introduction of new features.\n","TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n","Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n","\n","For more information see: https://github.com/tensorflow/addons/issues/2807 \n","\n","  warnings.warn(\n"]}],"source":["from tensorflow import keras \n","import tensorflow as tf \n","from tensorflow.keras import models \n","import os \n","#from imutils import paths \n","import matplotlib.pyplot as plt \n","import numpy as np \n","\n","import shutil\n","import cv2\n","import random\n","from dataclasses import dataclass \n","from tqdm import tqdm\n","import tempfile\n","from tensorflow.keras.layers import Dense, Input, Conv2D, MaxPooling2D, GlobalMaxPooling2D, \\\n","                                                GlobalAveragePooling2D, BatchNormalization, Flatten, ReLU\n","import tensorflow_addons as tfa  "]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-05-11T12:19:40.789495Z","iopub.status.busy":"2023-05-11T12:19:40.788017Z","iopub.status.idle":"2023-05-11T12:19:40.794325Z","shell.execute_reply":"2023-05-11T12:19:40.793008Z","shell.execute_reply.started":"2023-05-11T12:19:40.789459Z"},"trusted":true},"outputs":[],"source":["BATCH_SIZE = 128"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["class DataLoader: \n","    \"\"\"\n","        Class, will be useful for creating the BYOL dataset or dataset for the DownStream task \n","            like classification or segmentation.\n","        Methods:\n","            __download_data(scope: private)\n","            __normalize(scope: private)\n","            __preprocess_img(scope: private)\n","            __reshape_downstream_img(scope: private)\n","             __get_valdata(scope: private)\n","            get_byol_dataset(scope: public)\n","            get_downstream_data(scope: public)\n","        \n","        Property:\n","            dname(dtype: str)        : dataset name(supports cifar10, cifar100).\n","            byol_augmentor(type      : ByolAugmentor): byol augmentor instance/object.\n","            nval(type: int)          : Number of validation data needed, this will be created by splitting the testing\n","                                       data.\n","            resize_shape(dtype: int) : Resize shape, bcoz pretrained models, might have a different required shape.\n","            normalize(dtype: bool)   : bool value, whether to normalize the data or not. \n","    \"\"\"\n","    \n","    def __init__(self, dname=\"cifar10\", byol_augmentor=None, nval=5000,\n","                                             resize_shape=32, normalize=True, downstream_data=False): \n","        assert (byol_augmentor != None or downstream_data), 'Need a BYOL Augment object'\n","        assert dname in [\"cifar10\", 'cifar100'], \"dname should be either cifar10 or cifar100\"\n","        assert nval <= 10_000, \"ValueError: nval value should be <= 10_000\"\n","        \n","        __train_data, __test_data = self.__download_data(dname)\n","        self.__train_X, self.__train_y = __train_data\n","        self.__train_X, self.__train_y = self.__train_X, self.__train_y\n","      #  self.__train_X, self.__train_y = self.__train_X[: 100], self.__train_y[: 100]\n","        self.__dtest_X, self.__dtest_y = __test_data \n","        self.class_name = ['airplane', 'automobile', 'bird', 'cat', 'deer', \n","                                           'dog', 'frog', 'horse', 'sheep', 'truck']\n","        self.byol_augmentor = byol_augmentor\n","        self.__get_valdata(nval)\n","        self.resize_shape = resize_shape\n","        \n","        self.__normalize() if normalize else None\n","        self.min_obj_cov_value = 0.7\n","        self.color_jitter_value = 0.1\n","        \n","    def __len__(self): \n","        return self.__train_X.shape[0] + self.__dtest_X.shape[0]\n","    \n","    def __repr__(self): \n","        return f\"Training Samples: {self.__train_X.shape[0]}, Testing Samples: {self.__dtest_X.shape[0]}\"\n","    \n","    def __download_data(self, dname):\n","        \"\"\"\n","            Downloads the data from the tensorflow website using the tensorflw.keras.load_data() method.\n","            Params:\n","                dname(type: Str): dataset name, it just supports two dataset cifar10 or cifar100\n","            Return(type(np.ndarray, np.ndarray))\n","                returns the training data and testing data\n","        \"\"\"\n","        if dname == \"cifar10\": \n","            train_data, test_data = tf.keras.datasets.cifar10.load_data()\n","        if dname == \"cifar100\": \n","            train_data, test_data = tf.keras.datasets.cifar100.load_data()\n","            \n","        return train_data, test_data\n","    \n","    def __normalize(self): \n","        \"\"\"\n","            this method, will used to normalize the inputs.\n","        \"\"\"\n","        self.__train_X = self.__train_X / 255.0\n","        self.__dtest_X = self.__dtest_X / 255.0\n","    \n","    def __preprocess_img(self, image): \n","        \"\"\"\n","            this method, will be used by the get_byol_dataset methos, which does a convertion of \n","            numpy data to tensorflow data.\n","            Params:\n","                image(type: np.ndarray): image data.\n","            Returns(type; (np.ndarray, np.ndarray))\n","                returns the two different augmented views of same image.\n","        \"\"\"\n","        try: \n","            image = tf.image.convert_image_dtype(image, tf.float32)\n","            image = tf.image.resize(image, (self.resize_shape, self.resize_shape))\n","            view1 = self.byol_augmentor.augment(image, self.resize_shape)\n","            view2 = self.byol_augmentor.augment(image, self.resize_shape)\n","            \n","            return (view1, view2)\n","        \n","        except Exception as err:\n","            return err\n","    \n","    def get_byol_dataset(self, batch_size, dataset_type=\"train\"):\n","        \"\"\"\n","            this method, will gives the byol dataset, which is nothing but a tf.data.Dataset object.\n","            Params:\n","                batch_size(dtype: int)    : Batch Size.\n","                dataset_type(dtype: str)  : which type of dataset needed, (train, test or val)\n","                \n","            return(type: tf.data.Dataset)\n","                returns the tf.data.Dataset for intended dataset_type, by preprocessing and converting \n","                the np data.\n","        \"\"\"\n","        try:\n","            if dataset_type == \"train\":\n","                tensorflow_data = tf.data.Dataset.from_tensor_slices((self.__train_X))\n","                tensorflow_data = (\n","                tensorflow_data\n","                    .map(self.__preprocess_img, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n","                    .shuffle(1024)\n","                    .batch(batch_size, drop_remainder=True)\n","                    .prefetch(tf.data.experimental.AUTOTUNE)\n","                )\n","                return tensorflow_data  \n","            \n","            if dataset_type == \"test\":\n","                tensorflow_data = tf.data.Dataset.from_tensor_slices((self.__test_X))\n","                tensorflow_data = (\n","                tensorflow_data\n","                    .map(self.__preprocess_img, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n","                    .shuffle(1024)\n","                    .batch(batch_size, drop_remainder=True)\n","                    .prefetch(tf.data.experimental.AUTOTUNE)\n","                )\n","                return tensorflow_data  \n","            \n","            if dataset_type == \"val\":\n","                tensorflow_data = tf.data.Dataset.from_tensor_slices((self.__val_X))\n","                tensorflow_data = (\n","                tensorflow_data\n","                    .map(self.__preprocess_img, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n","                    .shuffle(1024)\n","                    .batch(batch_size, drop_remainder=True)\n","                    .prefetch(tf.data.experimental.AUTOTUNE)\n","                )\n","                return tensorflow_data  \n","        \n","        except Exception as err:\n","            return err\n","    \n","    def get_downstream_data(self): \n","        \"\"\"\n","            this method returns the dataset for the downstream task.\n","        \"\"\"\n","        return (self.__train_X, self.__train_y)#, (self.__val_X, self.__val_y), (self.__test_X, self.__test_y)\n","    \n","    def __get_valdata(self, nval):\n","        \"\"\"\n","            this method is used to create a validation data by randomly sampling from the testing data.\n","            Params:\n","                nval(dtype: Int); Number of validation data needed, rest of test_X.shape[0] - nval, will be \n","                                  testing data size.\n","            returns(type; np.ndarray, np.ndarray):\n","                returns the testing and validation dataset.\n","        \"\"\"\n","        try: \n","            ind_arr = np.arange(10_000)\n","            val_inds = np.random.choice(ind_arr, nval, replace=False)\n","            test_inds = [i for i in ind_arr if not i in val_inds]\n","\n","            self.__test_X, self.__test_y = self.__dtest_X[test_inds], self.__dtest_y[test_inds]\n","            self.__val_X, self.__val_y = self.__dtest_X[val_inds], self.__dtest_y[val_inds]\n","            \n","        except Exception as err:\n","            raise err    \n","            \n","    def __reshape_downstream_img(self, img, y):\n","        \"\"\"\n","            this method is used to reshape the image, and this method will be used by the get_downstream_tf_dataset\n","                method.\n","            Params:\n","                img(type: tf.Tensor): Image Tensor.\n","                y(dtype: int): Corresponding label of the image.\n","            Return(type: tf.Tensor, int)\n","                returns reshaped image and its label\n","        \"\"\"\n","        img = tf.image.resize(img, (self.resize_shape, self.resize_shape))\n","        return img, y\n","        \n","    def get_downstream_tf_dataset(self, batch_size, dataset_type=\"train\"): \n","        \"\"\"\n","             this method, will gives the downstream dataset, which is of type tf.data.Dataset object.\n","            Params:\n","                batch_size(dtype: int)    : Batch Size.\n","                dataset_type(dtype: str)  : which type of dataset needed, (train, test or val)\n","                \n","            return(type: tf.data.Dataset)\n","                returns the tf.data.Dataset for intended dataset_type, by preprocessing and converting \n","                the np data.\n","        \"\"\"\n","        assert dataset_type in [\"train\", \"test\", \"val\"], \"Given dataset type is not valid\"\n","        try:\n","            if dataset_type == \"train\":\n","                tensorflow_data = tf.data.Dataset.from_tensor_slices((self.__train_X, self.__train_y))\n","                tensorflow_data = (\n","                tensorflow_data\n","                    .map(self.__reshape_downstream_img, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n","                    .shuffle(1024)\n","                    .batch(batch_size, drop_remainder=True)\n","                    .prefetch(tf.data.experimental.AUTOTUNE)\n","                )\n","                return tensorflow_data  \n","            \n","            if dataset_type == \"test\":\n","                tensorflow_data = tf.data.Dataset.from_tensor_slices((self.__test_X, self.__test_X))\n","                tensorflow_data = (\n","                tensorflow_data\n","                    .map(self.__reshape_downstream_img, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n","                    .shuffle(1024)\n","                    .batch(batch_size, drop_remainder=True)\n","                    .prefetch(tf.data.experimental.AUTOTUNE)\n","                )\n","                return tensorflow_data  \n","            \n","            if dataset_type == \"val\":\n","                tensorflow_data = tf.data.Dataset.from_tensor_slices((self.__val_X, self.__val_y))\n","                tensorflow_data = (\n","                tensorflow_data\n","                    .map(self.__reshape_downstream_img, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n","                    .shuffle(1024)\n","                    .batch(batch_size, drop_remainder=True)\n","                    .prefetch(tf.data.experimental.AUTOTUNE)\n","                )\n","                return tensorflow_data  \n","        \n","        except Exception as err:\n","            return err"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["class BarlowTwinAugmentor: \n","    \"\"\"\n","        This class is used for the data augmentation for the byol model.\n","        Methods: \n","            __random_crop_flip_resize(scope: private)\n","            __random_color_distortion(scope: private)\n","            augment(scope: public)\n","    \"\"\"\n","    def __init__(self): \n","        pass\n","    \n","    @tf.function\n","    def __random_crop_resize(self, image, resize_shape):\n","        \"\"\"\n","            this method does a random crop with height and width of the crop are sampled randomly. it does the \n","            crop with the height and width, then it does a resizing again to the original shape. And also it does\n","            a flip of the image\n","            Params:\n","                image(type: tf.Tensor)   : image data of type tensor.\n","                resize_shape(type: int)  : Size of the image.\n","            Return(type: tf.Tensor)\n","                returns the crop and resized image.\n","        \"\"\"\n","        try: \n","            rand_size = tf.random.uniform(\n","                shape=[],\n","                minval=int(0.75 * resize_shape),\n","                maxval=1 * resize_shape,\n","                dtype=tf.int32)\n","\n","            crop = tf.image.random_crop(image, (rand_size, rand_size, 3))\n","            crop_resize = tf.image.resize(crop, (resize_shape, resize_shape))\n","            return crop_resize\n","\n","        except Exception as err:\n","            return err\n","    \n","    @tf.function\n","    def __random_flip(self, image):\n","        \"\"\"\n","            this method, will be used to do the random flip of the image, with 0.8 probability of \n","            chance.\n","            Params:\n","                image(type: tf.Tensor)   : image data of type tensor.\n","            Return(type: tf.Tensor)\n","                returns the Flipped image.\n","                \n","        \"\"\"\n","        try:\n","            random_val = tf.random.uniform(shape=[])\n","            if random_val < 0.8:\n","                image = tf.image.random_flip_left_right(image)\n","            return image\n","    \n","        except Exception as err:\n","            return err\n","    \n","    @tf.function\n","    def __random_color_distortion(self, image):\n","        \"\"\"\n","            this method, will do the color disortion augmentation for the given image.\n","            Params:\n","                image(type: tf.Tensor)   : image data of type tensor.\n","            Return(type: tf.Tensor)\n","                returns the random colored disorted image.\n","        \"\"\"\n","        try: \n","            color_jitter = tf.random.uniform(shape=[])\n","            if color_jitter < 0.8:\n","                image = tf.image.random_brightness(image, max_delta=0.8)\n","                image = tf.image.random_contrast(image, lower=0.4, upper=1.6)\n","                image = tf.image.random_saturation(image, lower=0.4, upper=164)\n","                image = tf.image.random_hue(image, max_delta=0.2)\n","                image = tf.clip_by_value(image, 0, 1)\n","            return image\n","        \n","        except Exception as error:\n","            return error\n","    \n","    @tf.function\n","    def __random_grayscale(self, image): \n","        \"\"\"\n","            this method, will convert the image into grayscale, with probability of 0.8.\n","            Params:\n","                image(type: tf.Tensor)   : image data of type tensor.\n","            Return(type: tf.Tensor)\n","                returns the randomly grayscale image.\n","        \"\"\"\n","        try: \n","            color_drop = tf.random.uniform(shape=[])\n","            if color_drop < 0.2:\n","                image = tf.image.rgb_to_grayscale(image)\n","                image = tf.tile(image, [1, 1, 3])\n","\n","            return image\n","        \n","        except Exception as err:\n","            return err\n","    \n","    @tf.function\n","    def __random_solarization(self, image): \n","        \"\"\"\n","            this method, will convert the image into solarization, with probability of 0.8.\n","            Params:\n","                image(type: tf.Tensor)   : image data of type tensor.\n","            Return(type: tf.Tensor)\n","                returns the randomly solarized image.\n","        \"\"\"\n","        try: \n","            random_val = tf.random.uniform(shape=[])\n","            if random_val < 0.2: \n","                image = tf.where(image < 10, image, 255 - image)\n","            return image\n","        \n","        except Exception as err:\n","            return err\n","    \n","    @tf.function\n","    def __random_gaussian_blur(self, image): \n","        \"\"\"\n","            this method, will convert the image into gaussian blured img, with probability of 0.8.\n","            Params:\n","                image(type: tf.Tensor)   : image data of type tensor.\n","            Return(type: tf.Tensor)\n","                returns the randomly blured image.\n","        \"\"\"\n","        try: \n","            random_val = tf.random.uniform(shape=[])\n","            if random_val < 0.2:\n","                s = np.random.random()\n","                return tfa.image.gaussian_filter2d(image=image, sigma=s)\n","            return image\n","        \n","        except Exception as err:\n","            return err\n","        \n","    def augment(self, image, resize_shape): \n","        \"\"\"\n","            this method will include all the augmentation as a pipeline(random crop, random flip, resize, and \n","            color disortion), this augment method will be used by DataLoader class.\n","            Params:\n","                image(type: tf.Tensor)   : image data of type tensor.\n","                resize_shape(type: int)  : Size of the image.\n","            Return(type: tf.Tensor)\n","                returns the preprocessed image.\n","                \n","        \"\"\"\n","        try: \n","            image = self.__random_crop_resize(image, 32)\n","            image = self.__random_flip(image)\n","            image = self.__random_color_distortion(image)\n","            image = self.__random_gaussian_blur(image)\n","            image = self.__random_grayscale(image)\n","            image = self.__random_solarization(image)\n","\n","            return image\n","        \n","        except Exception as error:\n","            print(error, error)\n","            return error"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["bt_augmentor = BarlowTwinAugmentor()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["bt_dataloader = DataLoader(\"cifar10\", bt_augmentor)\n","train_ds = bt_dataloader.get_byol_dataset(BATCH_SIZE, \"train\")\n","train_ds"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def visualize(train_ds):\n","    for batch in train_ds.take(1):\n","        pass\n","\n","    plt.figure(figsize=(7, 7))\n","\n","    ax1 = plt.subplot(2, 2, 1)\n","    ax1.grid(False)\n","    plt.imshow(batch[0][0].numpy().astype('float32'), interpolation = 'none', vmin = 0, vmax = 1)\n","    ax2 = plt.subplot(2, 2, 2)\n","    ax2.grid(False)\n","    plt.imshow(batch[1][0].numpy().astype('float32'), interpolation = 'none', vmin = 0, vmax = 1)\n","\n","    ax3 = plt.subplot(2, 2, 3)\n","    plt.hist(batch[0][0].numpy().ravel())\n","    ax4 = plt.subplot(2, 2, 4, sharey = ax3, sharex=ax3)\n","    plt.hist(batch[1][0].numpy().ravel())\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["visualize(train_ds)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class ResNet18:\n","    \"\"\"Resnet34 class.\n","\n","        Responsible for the Resnet 34 architecture.\n","    Modified from\n","    https://www.analyticsvidhya.com/blog/2021/08/how-to-code-your-resnet-from-scratch-in-tensorflow/#h2_2.\n","    https://www.analyticsvidhya.com/blog/2021/08/how-to-code-your-resnet-from-scratch-in-tensorflow/#h2_2.\n","        View their website for more information.\n","    \"\"\"\n","\n","    def identity_block(self, x, filter):\n","        # copy tensor to variable called x_skip\n","        x_skip = x\n","        # Layer 1\n","        x = tf.keras.layers.Conv2D(filter, (3, 3), padding=\"same\")(x)\n","        x = tf.keras.layers.BatchNormalization(axis=3)(x)\n","        x = tf.keras.layers.Activation(\"relu\")(x)\n","        # Layer 2\n","        x = tf.keras.layers.Conv2D(filter, (3, 3), padding=\"same\")(x)\n","        x = tf.keras.layers.BatchNormalization(axis=3)(x)\n","        # Add Residue\n","        x = tf.keras.layers.Add()([x, x_skip])\n","        x = tf.keras.layers.Activation(\"relu\")(x)\n","        return x\n","\n","    def convolutional_block(self, x, filter):\n","        # copy tensor to variable called x_skip\n","        x_skip = x\n","        # Layer 1\n","        x = tf.keras.layers.Conv2D(filter, (3, 3), padding=\"same\", strides=(2, 2))(x)\n","        x = tf.keras.layers.BatchNormalization(axis=3)(x)\n","        x = tf.keras.layers.Activation(\"relu\")(x)\n","        # Layer 2\n","        x = tf.keras.layers.Conv2D(filter, (3, 3), padding=\"same\")(x)\n","        x = tf.keras.layers.BatchNormalization(axis=3)(x)\n","        # Processing Residue with conv(1,1)\n","        x_skip = tf.keras.layers.Conv2D(filter, (1, 1), strides=(2, 2))(x_skip)\n","        # Add Residue\n","        x = tf.keras.layers.Add()([x, x_skip])\n","        x = tf.keras.layers.Activation(\"relu\")(x)\n","        return x\n","\n","    def __call__(self, shape=(32, 32, 3)):\n","        # Step 1 (Setup Input Layer)\n","        x_input = tf.keras.layers.Input(shape)\n","        x = tf.keras.layers.ZeroPadding2D((3, 3))(x_input)\n","        # Step 2 (Initial Conv layer along with maxPool)\n","        x = tf.keras.layers.Conv2D(64, kernel_size=7, strides=2, padding=\"same\")(x)\n","        x = tf.keras.layers.BatchNormalization()(x)\n","        x = tf.keras.layers.Activation(\"relu\")(x)\n","        x = tf.keras.layers.MaxPool2D(pool_size=3, strides=2, padding=\"same\")(x)\n","        # Define size of sub-blocks and initial filter size\n","        block_layers = [2, 2, 2, 2]\n","        filter_size = 64\n","        # Step 3 Add the Resnet Blocks\n","        for i in range(4):\n","            if i == 0:\n","                # For sub-block 1 Residual/Convolutional block not needed\n","                for j in range(block_layers[i]):\n","                    x = self.identity_block(x, filter_size)\n","            else:\n","                # One Residual/Convolutional Block followed by Identity blocks\n","                # The filter size will go on increasing by a factor of 2\n","                filter_size = filter_size * 2\n","                x = self.convolutional_block(x, filter_size)\n","                for j in range(block_layers[i] - 1):\n","                    x = self.identity_block(x, filter_size)\n","        # Step 4 End Dense Network\n","        x = tf.keras.layers.AveragePooling2D((2, 2), padding=\"same\")(x)\n","        x = tf.keras.layers.Flatten()(x)\n","        model = tf.keras.models.Model(inputs=x_input, outputs=x, name=\"ResNet34\")\n","        return model"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["class BarlowLoss(keras.losses.Loss):\n","    \"\"\"\n","        this class used for the barlow loss. It uses the cross correlation matrix, with the \n","            Invariance term and the Redudancy reduction term.\n","        methods:\n","            __get_off_diag(scope: private)\n","            __cross_corr_matrix_loss(scope: private)\n","            __normalize(scope: private)\n","            __cross_corr_matrix(scope: private)\n","        \n","        Property:\n","            lambda(dtype: float)     : Lambda, constant value for trade off between the invariance and \n","                redudant rediction.\n","            batch_size(dtype: int)   ; Number of batch\n","    \"\"\"\n","\n","    def __init__(self, batch_size: int):\n","        super(BarlowLoss, self).__init__()\n","        self.lambda_amt = 5e-3\n","        self.batch_size = batch_size\n","\n","    def __get_off_diag(self, c):\n","        \"\"\"\n","            Makes the diagonals of the cross correlation matrix zeros.\n","            Params:\n","                c(type: tf.Tensor): Cross correlation mstrix(N*D).\n","\n","            Returns(type: tf.Tensor):\n","                Returns a tf.tensor which represents the cross correlation\n","                matrix with its diagonals as zeros.\n","        \"\"\"\n","\n","        zero_diag = tf.zeros(c.shape[-1])\n","        return tf.linalg.set_diag(c, zero_diag)\n","\n","    def __cross_corr_matrix_loss(self, c: tf.Tensor):\n","        \"\"\"\n","            Gets the loss based on the cross correlation matrix. We want the diagonals to be 1's \n","            and everything else to be zeros to show that the two augmented images are similar.\n","            Params:\n","                c(type: tf.Tensor): Cross correlation mstrix(N*D)\n","\n","            Returns(type: tf.Tensor):\n","                Returns a tf.tensor which represents the cross correlation\n","                matrix with its diagonals as zeros.\n","        \"\"\"\n","\n","        # subtracts diagonals by one and squares them(first part)\n","        c_diff = tf.pow(tf.linalg.diag_part(c) - 1, 2)\n","\n","        # takes off diagonal, squares it, multiplies with lambda(second part)\n","        off_diag = tf.pow(self.__get_off_diag(c), 2) * self.lambda_amt\n","\n","        # sum first and second parts together\n","        loss = tf.reduce_sum(c_diff) + tf.reduce_sum(off_diag)\n","\n","        return loss\n","\n","    def __normalize(self, output):\n","        \"\"\"\n","        this method, will do the batch normaliztion of the input embeddings, without a batch\n","        normalization, the model produces a bad result.\n","        Params:\n","            output(dtype: tf.tensor): the model prediction.\n","\n","        Returns(dtype: tf.Tensor):\n","            Returns a normalized version of the model prediction.\n","        \"\"\"\n","\n","        return (output - tf.reduce_mean(output, axis=0)) / tf.math.reduce_std(\n","            output, axis=0\n","        )\n","\n","    def __cross_corr_matrix(self, z_a_norm, z_b_norm):\n","        \"\"\"cross_corr_matrix method.\n","\n","        Creates a cross correlation matrix from the predictions.\n","        It transposes the first prediction and multiplies this with\n","        the second, creating a matrix with shape (n_dense_units, n_dense_units).\n","        See build_twin() for more info. Then it divides this with the\n","        batch size.\n","\n","        Arguments:\n","            z_a_norm: A normalized version of the first prediction.\n","            z_b_norm: A normalized version of the second prediction.\n","\n","        Returns:\n","            Returns a cross correlation matrix.\n","        \"\"\"\n","        return (tf.transpose(z_a_norm) @ z_b_norm) / self.batch_size\n","    \n","    @tf.autograph.experimental.do_not_convert\n","    def call(self, z_a: tf.Tensor, z_b: tf.Tensor) :\n","        \"\"\"call method.\n","\n","        Makes the cross-correlation loss. Uses the CreateCrossCorr\n","        class to make the cross corr matrix, then finds the loss and\n","        returns it(see cross_corr_matrix_loss()).\n","\n","        Arguments:\n","            z_a: The prediction of the first set of augmented data.\n","            z_b: the prediction of the second set of augmented data.\n","\n","        Returns:\n","            Returns a (rank-0) tf.Tensor that represents the loss.\n","        \"\"\"\n","\n","        z_a_norm, z_b_norm = self.__normalize(z_a), self.__normalize(z_b)\n","        c = self.__cross_corr_matrix(z_a_norm, z_b_norm)\n","        loss = self.__cross_corr_matrix_loss(c)\n","        return loss"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def get_projector(input_dim, hidden_dims1, \n","                        hidden_dims2, hidden_dims3):\n","    \"\"\"\n","        this function, build the Projection newtork(g), for one view of the network\n","        Params:\n","            input_dim (dtye; int)          : Input vector dimensionality\n","            l2_reg_penalty (dtype; float)  : L2 penalty value.\n","            hidden_dims1(dtype: Int)       : Hidden layer1 neuron\n","            hidden_dims2 (dtype: int)      : Hidden layer2 neuron\n","\n","        Return(type; keras.models.Model):\n","            The keras model of the Projection network(g)\n","    \"\"\"\n","      \n","    _input = Input(input_dim, name='Projection input1')\n","  \n","    x = Dense(hidden_dims1, name=\"dense1\")(_input)\n","    x = BatchNormalization(name='bn1')(x)\n","    x = ReLU(name='relu1')(x)\n","    x = Dense(hidden_dims2, name=\"dense2\")(_input)\n","    x = BatchNormalization(name='bn2')(x)\n","    x = ReLU(name='relu2')(x)\n","    _output = Dense(hidden_dims3, name='dense3')(x)\n","\n","    return keras.models.Model(_input, _output, name='Projection')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["projection = get_projector(2048, 8192, 8192, 8192)\n","projection.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def get_encoder_projector(input_shape, hidden_dims1, \n","                                hidden_dims2, hidden_dims3):\n","    \"\"\"\n","        this function, will build a encoder(f) with the projection(g)\n","        Params:\n","            input_shape (dtye; tuple)      : Input image dimension\n","            hidden_dims3 (dtype; Int)      : Hidden layer3 neurons, for the projection(q) model.\n","            hidden_dims1(dtype: Int)       : Hidden layer1 neurons, for the projection(q) model.\n","            hidden_dims2 (dtype: int)      : Hidden layer2 neurons, for the projection(q) model.\n","\n","        Return(type; keras.models.Model):\n","            The keras model of the Projection network(g)\n","    \"\"\"\n","    encoder = ResNet18()()\n","    last_layer = encoder.layers[-1].output\n","    \n","    projection_input_dims = last_layer.shape[-1]\n","    projection = get_projector(projection_input_dims, hidden_dims1, hidden_dims2, hidden_dims3)\n","    embeddings = projection(last_layer)\n","    \n","    return keras.models.Model(encoder.input, embeddings, name=\"Barlow-Twin\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["enc_proj_model = get_encoder_projector((32, 32, 3), 2048, 2048, 2048)\n","enc_proj_model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["class BarlowModel(keras.Model):\n","    \"\"\"BarlowModel class.\n","\n","    BarlowModel class. Responsible for making predictions and handling\n","    gradient descent with the optimizer.\n","\n","    Attributes:\n","        model: the barlow model architecture.\n","        loss_tracker: the loss metric.\n","\n","    Methods:\n","        train_step: one train step; do model predictions, loss, and\n","            optimizer step.\n","        metrics: Returns metrics.\n","    \"\"\"\n","\n","    def __init__(self):\n","        super(BarlowModel, self).__init__()\n","        self.model = get_encoder_projector((32, 32, 3), 2048, 2048, 2048)\n","        self.loss_tracker = keras.metrics.Mean(name=\"loss\")\n","\n","    @property\n","    def metrics(self):\n","        return [self.loss_tracker]\n","\n","    def train_step(self, inputs: tf.Tensor) -> tf.Tensor:\n","        \"\"\"train_step method.\n","\n","        Do one train step. Make model predictions, find loss, pass loss to\n","        optimizer, and make optimizer apply gradients.\n","\n","        Arguments:\n","            batch: one batch of data to be given to the loss function.\n","\n","        Returns:\n","            Returns a dictionary with the loss metric.\n","        \"\"\"\n","        view1, view2 = inputs\n","        with tf.GradientTape() as tape:\n","            z_a = self.model(view1, training=True)\n","            z_b = self.model(view2, training=True)\n","            loss = self.loss(z_a, z_b)\n","        \n","        params = self.model.trainable_variables\n","        grads = tape.gradient(loss, params)\n","\n","        self.optimizer.apply_gradients(zip(grads, params))\n","        self.loss_tracker.update_state(loss)\n","\n","        return {\"loss\": self.loss_tracker.result()}"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2023-05-11T12:21:11.667023Z","iopub.status.busy":"2023-05-11T12:21:11.663718Z","iopub.status.idle":"2023-05-11T12:21:11.682678Z","shell.execute_reply":"2023-05-11T12:21:11.681641Z","shell.execute_reply.started":"2023-05-11T12:21:11.666974Z"},"trusted":true},"outputs":[],"source":["class WarmUp(tf.keras.optimizers.schedules.LearningRateSchedule):\n","    def __init__(self,\n","               initial_learning_rate: float,\n","               decay_schedule_fn,\n","               warmup_steps: int,\n","               power: float = 1.0,\n","               name: str = None,):\n","\n","        super().__init__()\n","        self.initial_learning_rate = initial_learning_rate\n","        self.warmup_steps = warmup_steps\n","        self.power = power\n","        self.decay_schedule_fn = decay_schedule_fn\n","        self.name = name\n","\n","    def __call__(self, step):\n","        with tf.name_scope(self.name or \"WarmUp\") as name:\n","            # Implements polynomial warmup. i.e., if global_step < warmup_steps, the\n","            # learning rate will be `global_step/num_warmup_steps * init_lr`.\n","            global_step_float = tf.cast(step, tf.float32)\n","            warmup_steps_float = tf.cast(self.warmup_steps, tf.float32)\n","            warmup_percent_done = global_step_float / warmup_steps_float\n","            warmup_learning_rate = self.initial_learning_rate * tf.math.pow(warmup_percent_done, self.power)\n","            return tf.cond(\n","                global_step_float < warmup_steps_float,\n","                lambda: warmup_learning_rate,\n","                lambda: self.decay_schedule_fn(step - self.warmup_steps),\n","                name=name,\n","            )\n","\n","    def get_config(self):\n","        return {\n","                \"initial_learning_rate\": self.initial_learning_rate,\n","                \"decay_schedule_fn\": self.decay_schedule_fn,\n","                \"warmup_steps\": self.warmup_steps,\n","                \"power\": self.power,\n","                \"name\": self.name,\n","                }"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2023-05-11T12:21:12.035152Z","iopub.status.busy":"2023-05-11T12:21:12.032421Z","iopub.status.idle":"2023-05-11T12:21:12.063430Z","shell.execute_reply":"2023-05-11T12:21:12.062419Z","shell.execute_reply.started":"2023-05-11T12:21:12.035113Z"},"trusted":true},"outputs":[],"source":["decay_steps = (len(train_ds))*1000\n","warmup_steps = (len(train_ds))*10\n","initial_lr = 5e-4 \n","\n","lr_decayed_fn = tf.keras.experimental.CosineDecay(initial_learning_rate = initial_lr, \n","                                                decay_steps = decay_steps)\n","\n","cosine_with_warmUp = WarmUp(initial_learning_rate = initial_lr,\n","                          decay_schedule_fn = lr_decayed_fn,\n","                          warmup_steps = warmup_steps)\n","\n","optimizer = keras.optimizers.Adam(cosine_with_warmUp)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["bt_model = BarlowModel()\n","bt_loss = BarlowLoss(BATCH_SIZE)\n","\n","bt_model.compile(optimizer=optimizer, loss=bt_loss)"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-output":true,"scrolled":true,"trusted":true},"outputs":[],"source":["bt_model.fit(train_ds, epochs=100)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[" bt_model.model.save(\"barlow-twin\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.10"}},"nbformat":4,"nbformat_minor":4}
